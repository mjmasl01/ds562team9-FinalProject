{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, avg\n",
        "\n",
        "# Start Spark session (if not already started)\n",
        "spark = SparkSession.builder.appName(\"SentimentAgg\").getOrCreate()\n",
        "\n",
        "# Set storage account config\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")\n",
        "\n",
        "df_sentiments = spark.read.csv(\n",
        "    \"wasbs://silver@ds562team9datalake.blob.core.windows.net/CSV_files/Final_sentiments.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Print schema\n",
        "df_sentiments.printSchema()\n",
        "from pyspark.sql.functions import weekofyear, year\n",
        "\n",
        "df_weekly_agg = df_sentiments.groupBy(\n",
        "    year(\"date\").alias(\"year\"),\n",
        "    weekofyear(\"date\").alias(\"week\")\n",
        ").agg(\n",
        "    avg(\"Close\").alias(\"avg_close\"),\n",
        "    avg(\"tweet_sentiment\").alias(\"avg_tweet_sentiment\"),\n",
        "    avg(\"news_sentiment\").alias(\"avg_news_sentiment\")\n",
        ")\n",
        "\n",
        "df_weekly_agg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, mean, isnan, when\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"ModelInputCheck\").getOrCreate()\n",
        "\n",
        "# Set Azure config\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")\n",
        "\n",
        "# Load the CSV\n",
        "df_model = spark.read.csv(\n",
        "    \"wasbs://silver@ds562team9datalake.blob.core.windows.net/CSV_files/Model_input_file.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Print schema\n",
        "df_model.printSchema()\n",
        "\n",
        "# Show sample data\n",
        "df_model.show(5, truncate=False)\n",
        "\n",
        "# -------------------------------\n",
        "# ðŸ“Š 2. SUMMARY STATISTICS\n",
        "# -------------------------------\n",
        "df_model.describe().show()\n",
        "\n",
        "# -------------------------------\n",
        "# ðŸ“ˆ 3. SIMPLE AGGREGATION EXAMPLE\n",
        "# -------------------------------\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df_model_agg = df_model.agg(\n",
        "    avg(\"tweet_sentiment\").alias(\"avg_tweet_sentiment\"),\n",
        "    avg(\"news_sentiment\").alias(\"avg_news_sentiment\"),\n",
        "    avg(\"sma_5\").alias(\"avg_sma_5\"),\n",
        "    avg(\"volatility_3d\").alias(\"avg_volatility_3d\")\n",
        ")\n",
        "\n",
        "df_model_agg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, mean, isnan, when, avg, year\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"CorrelationDataCheck\").getOrCreate()\n",
        "\n",
        "# Set Azure config\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")\n",
        "\n",
        "# Load the CSV\n",
        "df_corr = spark.read.csv(\n",
        "    \"wasbs://silver@ds562team9datalake.blob.core.windows.net/CSV_files/Spearman_Pearson_correlation.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Print schema\n",
        "df_corr.printSchema()\n",
        "\n",
        "# Show first 5 rows\n",
        "df_corr.show(5, truncate=False)\n",
        "\n",
        "# -------------------------------\n",
        "# ðŸ“Š SUMMARY STATISTICS\n",
        "# -------------------------------\n",
        "df_corr.describe().show()\n",
        "\n",
        "# -------------------------------\n",
        "# ðŸ“ˆ AGGREGATE SELECTED METRICS\n",
        "# -------------------------------\n",
        "df_corr_agg = df_corr.agg(\n",
        "    avg(\"tweet_sentiment\").alias(\"avg_tweet_sentiment\"),\n",
        "    avg(\"news_sentiment\").alias(\"avg_news_sentiment\"),\n",
        "    avg(\"volatility_3d\").alias(\"avg_volatility_3d\"),\n",
        "    avg(\"rsi_14\").alias(\"avg_rsi_14\")\n",
        ")\n",
        "df_corr_agg.show()\n",
        "\n",
        "# -------------------------------\n",
        "# ðŸ“… GROUP BY YEAR\n",
        "# -------------------------------\n",
        "df_corr_by_year = df_corr.groupBy(year(\"date\").alias(\"year\")).agg(\n",
        "    avg(\"Close\").alias(\"avg_close\"),\n",
        "    avg(\"tweet_sentiment\").alias(\"avg_tweet_sentiment\"),\n",
        "    avg(\"news_sentiment\").alias(\"avg_news_sentiment\"),\n",
        "    avg(\"volatility_3d\").alias(\"avg_volatility_3d\"),\n",
        "    avg(\"rsi_14\").alias(\"avg_rsi_14\")\n",
        ")\n",
        "df_corr_by_year.orderBy(\"year\").show()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}