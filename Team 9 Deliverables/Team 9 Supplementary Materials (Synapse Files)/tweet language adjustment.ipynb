{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Define storage account and container\n",
        "storage_account = \"ds562team9datalake\"\n",
        "container_name = \"silver\"\n",
        "file_path = \"historical_crypto_tweet_data/historical_crypto_tweets_2014_2019.parquet\"\n",
        "\n",
        "storage_key = \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        "\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", storage_key)\n",
        "spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
        "spark.conf.set(\n",
        "    \"spark.sql.sources.commitProtocolClass\",\n",
        "    \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    \"spark.sql.parquet.output.committer.class\",\n",
        "    \"org.apache.spark.sql.execution.datasources.parquet.DirectParquetOutputCommitter\"\n",
        ")\n",
        "\n",
        "url = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/{file_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.parquet(url)\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# This filters rows where the 'text' column contains only ASCII characters\n",
        "df_english = df.filter(col(\"text\").rlike(\"^[\\\\x00-\\\\x7F]+$\"))\n",
        "\n",
        "# Now count\n",
        "total_rows = df.count()\n",
        "english_rows = df_english.count()\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"English-like tweets (ASCII-only): {english_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_english.show(5, truncate=False)\n",
        "df_english.groupBy(\"year\").count().orderBy(\"year\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Fix the committer issue\n",
        "spark.conf.set(\"spark.sql.parquet.output.committer.class\", \"org.apache.parquet.hadoop.ParquetOutputCommitter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Then proceed to write\n",
        "df_english.coalesce(1).write.mode(\"overwrite\").parquet(\n",
        "    \"wasbs://bronze@ds562team9datalake.blob.core.windows.net/Historical_crypto_tweets/cleaned/\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}