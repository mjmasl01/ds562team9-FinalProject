{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Crypto News 2013-2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### data source for 2013-2018... https://www.kaggle.com/datasets/kashnitsky/news-about-major-cryptocurrencies-20132018-40k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Substitute with your actual info\n",
        "storage_account = \"ds562team9datalake\"\n",
        "container_name = \"bronze\"\n",
        "file_path = \"Historical_news_on_crypto/crypto_news_2013-2017.csv\"\n",
        "storage_key = \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\" \n",
        "\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", storage_key)\n",
        "\n",
        "url = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/{file_path}\"\n",
        "\n",
        "df_crypto_news_2013_2017 = spark.read.option(\"header\", True).csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "storage_account = \"ds562team9datalake\"\n",
        "container = \"bronze\"\n",
        "file_path = \"Historical_news_on_crypto/crypto_news_2013-2017.csv\"\n",
        "\n",
        "wasbs_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/{file_path}\"\n",
        "\n",
        "df_crypto_news_2013_2017 = spark.read.option(\"header\", True) \\\n",
        "               .option(\"multiline\", True) \\\n",
        "               .option(\"escape\", \"\\\"\") \\\n",
        "               .option(\"quote\", \"\\\"\") \\\n",
        "               .option(\"inferSchema\", False) \\\n",
        "               .csv(wasbs_path)\n",
        "\n",
        "df_crypto_news_2013_2017.printSchema()\n",
        "df_crypto_news_2013_2017.select(\"title\", \"year\", \"source\").show(10, truncate=True)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, trim\n",
        "\n",
        "missing_or_blank_count = df_crypto_news_2013_2017.filter(\n",
        "    (col(\"title\").isNull() | (trim(col(\"title\")) == \"\")) |\n",
        "    (col(\"text\").isNull() | (trim(col(\"text\")) == \"\")) |\n",
        "    (col(\"year\").isNull() | (trim(col(\"year\")) == \"\"))\n",
        ").count()\n",
        "print(\"raw data row count:\", df_crypto_news_2013_2017.count())\n",
        "print(f\"Rows with missing or blank 'title', 'text', or 'year': {missing_or_blank_count}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import length\n",
        "\n",
        "df_clean_crypto_news_2013_2017 = df_crypto_news_2013_2017.filter(\n",
        "    (col(\"title\").isNotNull()) & (trim(col(\"title\")) != \"\") &\n",
        "    (col(\"text\").isNotNull()) & (trim(col(\"text\")) != \"\") &\n",
        "    (col(\"year\").isNotNull()) & (trim(col(\"year\")) != \"\")\n",
        ")\n",
        "\n",
        "print(\"Cleaned row count:\", df_clean_crypto_news_2013_2017.count())\n",
        "print(\"Cleaned 2013-2017 row count:\", df_clean_crypto_news_2013_2017.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Crypto News 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {},
      "source": [
        "file_path_2018 = \"Historical_news_on_crypto/crypto_news_2018.csv\"\n",
        "wasbs_path_2018 = f\"wasbs://bronze@ds562team9datalake.blob.core.windows.net/{file_path_2018}\"\n",
        "\n",
        "df_2018 = spark.read.option(\"header\", True).option(\"multiline\", True).option(\"escape\", \"\\\"\").option(\"quote\", \"\\\"\").csv(wasbs_path_2018)\n",
        "\n",
        "df_2018.printSchema()\n",
        "df_2018.select(\"title\", \"year\", \"source\").show(10, truncate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {},
      "source": [
        "missing_2018 = df_2018.filter(\n",
        "    (col(\"title\").isNull() | (trim(col(\"title\")) == \"\")) |\n",
        "    (col(\"text\").isNull() | (trim(col(\"text\")) == \"\")) |\n",
        "    (col(\"year\").isNull() | (trim(col(\"year\")) == \"\"))\n",
        ").count()\n",
        "\n",
        "print(f\"Rows with missing or blank 'title', 'text', or 'year' in 2018 dataset: {missing_2018}\")\n",
        "\n",
        "df_2018_clean = df_2018.filter(\n",
        "    (col(\"title\").isNotNull()) & (trim(col(\"title\")) != \"\") &\n",
        "    (col(\"text\").isNotNull()) & (trim(col(\"text\")) != \"\") &\n",
        "    (col(\"year\").isNotNull()) & (trim(col(\"year\")) != \"\")\n",
        ")\n",
        "\n",
        "print(\"Original 2018 row count:\", df_2018_clean.count())\n",
        "print(\"Cleaned 2018 row count:\", df_2018_clean.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Combining 2013-2017 with 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_2013_2018_clean = df_clean_crypto_news_2013_2017.unionByName(df_2018_clean)\n",
        "\n",
        "print(\"Row count - 2013–2017:\", df_clean_crypto_news_2013_2017.count())\n",
        "print(\"Row count - 2018:\", df_2018_clean.count())\n",
        "print(\"Row count - Combined (2013–2018):\", df_2013_2018_clean.count())\n",
        "\n",
        "print(\"Combined (2013–2018):\", df_2013_2018_clean.describe())\n",
        "print(\"Columns:\", df_2013_2018_clean.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Combined Data 2013-2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, sum, when\n",
        "\n",
        "null_counts = df_2013_2018_clean.select([\n",
        "    sum(when(col(c).isNull() | (col(c) == \"\"), 1).otherwise(0)).alias(c)\n",
        "    for c in df_2013_2018_clean.columns\n",
        "])\n",
        "\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_final_trimmed = df_2013_2018_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "df_final_trimmed = df_final_trimmed.filter(\n",
        "    lower(col(\"title\")).contains(\"bitcoin\") | \n",
        "    lower(col(\"title\")).contains(\"btc\") | \n",
        "    lower(col(\"text\")).contains(\"bitcoin\") | \n",
        "    lower(col(\"text\")).contains(\"btc\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(\"Columns:\", df_final_trimmed.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_final_trimmed.coalesce(1).write.mode(\"overwrite\").parquet(\n",
        "    \"wasbs://bronze@ds562team9datalake.blob.core.windows.net/Historical_news_on_crypto/cleaned_news_data\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}