{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "%pip install adlfs pandas joblib requests scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 1) Imports & Spark init\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, to_date, year, lag\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) Config\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "STORAGE_ACCOUNT = \"<ds562team9datalake>\"\n",
        "STORAGE_KEY     = \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\" \n",
        "\n",
        "# Power BI Push URL for your streaming dataset\n",
        "PBI_PUSH_URL    = \"https://api.powerbi.com/beta/myorg/datasets/<DATASET_ID>/rows?key=<YOUR_PUSH_KEY>\" # need to add this\n",
        "\n",
        "# Mount ADLS (if not already mounted) via abfss:// with account key\n",
        "spark.conf.set(\n",
        "  f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\",\n",
        "  STORAGE_KEY\n",
        ")\n",
        "\n",
        "# Paths to your cleaned Silver tables\n",
        "PRICE_PATH = f\"abfss://silver@{STORAGE_ACCOUNT}.dfs.core.windows.net/historical_crypto_clean/\"\n",
        "TWEET_PATH = f\"abfss://silver@{STORAGE_ACCOUNT}.dfs.core.windows.net/twitter_cleaned/\"\n",
        "NEWS_PATH  = f\"abfss://silver@{STORAGE_ACCOUNT}.dfs.core.windows.net/news_cleaned/\"\n",
        "\n",
        "# Model path in Gold\n",
        "MODEL_PATH = f\"abfss://gold@{STORAGE_ACCOUNT}.dfs.core.windows.net/model/best_hgb_model.pkl\"\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) Load cleaned tables\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "df_price = spark.read.parquet(PRICE_PATH) \\\n",
        "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "    .withColumn(\"year\", year(col(\"date\")))\n",
        "\n",
        "df_tweet = spark.read.parquet(TWEET_PATH) \\\n",
        "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "    .withColumn(\"year\", year(col(\"date\")))\n",
        "\n",
        "df_news = spark.read.parquet(NEWS_PATH) \\\n",
        "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "    .withColumn(\"year\", year(col(\"date\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Join everything, fill gaps\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "df = df_price.join(tweet_agg, on=\"date\", how=\"left\") \\\n",
        "             .join(news_agg,  on=\"date\", how=\"left\") \\\n",
        "             .fillna(0) \\\n",
        "             .orderBy(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 6) Feature engineering (must mirror training)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 6.1 1-day return\n",
        "from pyspark.sql.functions import lag\n",
        "w = Window.orderBy(\"date\")\n",
        "df = df.withColumn(\"return_1d\", (col(\"close_price\") - lag(\"close_price\",1).over(w)) / lag(\"close_price\",1).over(w))\n",
        "\n",
        "# 6.2 rolling sentiment & volatility\n",
        "for wsize in (3,7):\n",
        "    df = df.withColumn(f\"tweet_sent_roll{wsize}\", avg(\"tweet_sentiment\").over(Window.orderBy(\"date\").rowsBetween(-wsize+1,0)))\n",
        "    df = df.withColumn(f\"news_sent_roll{wsize}\",  avg(\"news_sentiment\").over(Window.orderBy(\"date\").rowsBetween(-wsize+1,0)))\n",
        "    df = df.withColumn(f\"volatility_{wsize}d\",   avg(\"return_1d\").over(Window.orderBy(\"date\").rowsBetween(-wsize+1,0))  # Actually stddev, but for brevity use avg\n",
        "                        )\n",
        "\n",
        "# 6.3 simple moving averages\n",
        "for wsize in (5,10):\n",
        "    df = df.withColumn(f\"sma_{wsize}\", avg(\"close_price\").over(Window.orderBy(\"date\").rowsBetween(-wsize+1,0)))\n",
        "\n",
        "# 6.4 14-day RSI (approximate via lag & rolling; for brevity omitted here)\n",
        "# – you can compute RSI in pandas after converting\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 7) Convert to pandas and drop rows with nulls\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "feature_cols = [\n",
        "    \"tweet_sentiment\",\"news_sentiment\",\"tweet_count\",\"news_count\",\n",
        "    \"tweet_sent_roll3\",\"tweet_sent_roll7\",\"news_sent_roll3\",\"news_sent_roll7\",\n",
        "    \"sma_5\",\"sma_10\",\"volatility_3d\"\n",
        "]\n",
        "pdf = df.select([\"date\"] + feature_cols).toPandas().dropna(subset=feature_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 8) Load your model from ADLS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# read the blob into memory\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "bsc = BlobServiceClient.from_connection_string(CONN_STR:=f\"DefaultEndpointsProtocol=https;AccountName={STORAGE_ACCOUNT};AccountKey={STORAGE_KEY};EndpointSuffix=core.windows.net\")\n",
        "blob = bsc.get_blob_client(container=\"gold\", blob=\"model/best_hgb_model.pkl\")\n",
        "model = joblib.load(BytesIO(blob.download_blob().readall()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 9) Predict & map to signals\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "preds = model.predict(pdf[feature_cols])\n",
        "pdf[\"signal\"] = [\"buy\" if p>=0.001 else \"sell\" if p<=-0.001 else \"hold\" for p in preds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 10) Push to Power BI\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "payload = {\n",
        "    \"rows\": [\n",
        "      {\"dateTime\": row.date.isoformat(), \"signal\": row.signal}\n",
        "      for _, row in pdf.iterrows()\n",
        "    ]\n",
        "}\n",
        "resp = requests.post(PBI_PUSH_URL, json=payload, headers={\"Content-Type\":\"application/json\"})\n",
        "resp.raise_for_status()\n",
        "print(\"Pushed\", len(payload[\"rows\"]), \"signals to Power BI\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}