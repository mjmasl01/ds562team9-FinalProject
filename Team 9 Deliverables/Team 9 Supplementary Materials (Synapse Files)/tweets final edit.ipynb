{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Define storage account and container\n",
        "storage_account = \"ds562team9datalake\"\n",
        "container_name = \"silver\"\n",
        "file_path = \"historical_crypto_tweet_data/historical_crypto_tweets_2014_2019.parquet\"\n",
        "\n",
        "storage_key = \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        "\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", storage_key)\n",
        "spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
        "spark.conf.set(\n",
        "    \"spark.sql.sources.commitProtocolClass\",\n",
        "    \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    \"spark.sql.parquet.output.committer.class\",\n",
        "    \"org.apache.spark.sql.execution.datasources.parquet.DirectParquetOutputCommitter\"\n",
        ")\n",
        "\n",
        "url = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/{file_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_raw = spark.read.parquet(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_raw.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_raw.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_cleaned = df_raw.drop(\"sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_cleaned.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.ds562team9datalake.blob.core.windows.net\",\n",
        "    \"KXg2Djg7uRevBSpPNIVnKw/N6HpqBh+kJwDX07wkywbpU2joMZdTIBOXk30EoMMxH2d8wwb+9j0g+AStO60IWw==\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Fix the committer issue\n",
        "spark.conf.set(\"spark.sql.parquet.output.committer.class\", \"org.apache.parquet.hadoop.ParquetOutputCommitter\")\n",
        "\n",
        "# Then proceed to write\n",
        "df_cleaned.coalesce(1).write.mode(\"overwrite\").parquet(\n",
        "    \"wasbs://bronze@ds562team9datalake.blob.core.windows.net/Historical_crypto_tweets/cleaned\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}